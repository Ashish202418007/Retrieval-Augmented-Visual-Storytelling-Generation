# RAVSG: Story-Image Bidirectional Generator
 Studio Ghibli-inspired bidirectional story-image generation using SOTA open-source models with RAG enhancement.

### Authors : 
Ashish Kar(202418007)

Misty Roy(202418033)

Yashraj Sinh(202418064)

Pragnya Dandavate(202418065)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚ â† REST API
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â†’ Redis Queue â”€â”€â†’ Story Worker (LLaVA)
       â”‚                   â””â”€â†’ CLIP Embeddings
       â”‚                       â””â”€â†’ FAISS RAG
       â”‚
       â””â”€â”€â†’ Redis Queue â”€â”€â†’ Image Worker (Stable Diffusion)
                           â””â”€â†’ CLIP Embeddings
                               â””â”€â†’ FAISS RAG
```

## Models Used

* **Embeddings**: `openai/clip-vit-base-patch16` (512D multimodal embeddings)
* **Imageâ†’Story**: `llava-hf/llava-1.5-7b-hf`
* **Storyâ†’Image**: `runwayml/stable-diffusion-v1-5`
* **Vector DB**: FAISS (Flat or IVFFlat)

## Quick Start

### Local Setup

```bash
# Create virtual environment
python3.10 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start Redis
redis-server

# Start API server
python api/server.py

# Start workers (in separate terminals)
python workers/generation_worker.py story
python workers/generation_worker.py image
```

<!-- 
### Docker Setup

```bash
# Build and run all services
docker-compose up --build

# Scale workers
docker-compose up --scale story_worker=2 --scale image_worker=2 
```-->

## API Usage

### Generate Story from Image

```bash
# Submit job
curl -X POST http://localhost:8000/generate-story \
  -H "Content-Type: application/json" \
  -d '{"image": "data:image/png;base64,..."}'

# Response: {"job_id": "abc-123", "status": "pending"}

# Poll for results
curl http://localhost:8000/job/abc-123

# Response when complete:
{
  "job_id": "abc-123",
  "status": "completed",
  "result": {"story": "Once upon a time..."}
}
```

### Generate Image from Story

```bash
# Submit job
curl -X POST http://localhost:8000/generate-image \
  -H "Content-Type: application/json" \
  -d '{"story": "A magical forest at sunset..."}'

# Poll for results
curl http://localhost:8000/job/xyz-456

# Response:
{
  "job_id": "xyz-456",
  "status": "completed",
  "result": {"image": "data:image/png;base64,..."}
}
```

## Configuration

Edit `config/settings.py` or create `.env`:

```env
# Models
VISION_LANGUAGE_MODEL=llava-hf/llava-1.5-7b-hf
TEXT_TO_IMAGE_MODEL=runwayml/stable-diffusion-v1-5
EMBEDDING_MODEL=openai/clip-vit-base-patch16

# Performance
DEVICE=cuda
USE_FP16=true
USE_8BIT=false

# RAG
EMBEDDING_DIM=512
TOP_K_RETRIEVAL=5
FAISS_INDEX_TYPE=Flat   # Options: Flat, IVFFlat
FAISS_NLIST=100

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379

# Langfuse (optional)
ENABLE_LANGFUSE=false
LANGFUSE_PUBLIC_KEY=pk-...
LANGFUSE_SECRET_KEY=sk-...
```

## GPU Memory Management

For RTX-class GPUs:

1. **Single worker per GPU**: Each worker loads one model at a time
2. **Lazy loading**: Models loaded on-demand, unloaded after use
3. **FP16 precision**: Reduces memory footprint
4. **Optional 8-bit**: For memory-constrained environments

Approximate memory usage:

* CLIP ViT-B/16: ~1GB
* LLaVA 1.5 7B: ~13GB
* Stable Diffusion v1.5: ~6GB

## Project Structure

```
root
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ server.py              # FastAPI endpoints
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py            # Configuration
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ model_manager.py       # Model loading & inference
â”‚   â”‚   â”œâ”€â”€ rag_engine.py          # FAISS vector database
â”‚   â”‚   â”œâ”€â”€ queue_manager.py       # Redis job queue
â”‚   â”‚   â””â”€â”€ observability.py       # Langfuse tracing
â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â””â”€â”€ generation_worker.py   # Async job processor
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ image_utils.py         # Image encoding/decoding
â”‚   â”œâ”€â”€ database/                  # FAISS indices
â”‚   â”œâ”€â”€ models/                    # Model weights cache
â”‚   â”œâ”€â”€ media/                     # Generated content
â”‚   â””â”€â”€ logs/                      # Application logs
â”‚
â”œâ”€â”€ frontend
â”‚   â””â”€â”€ app.html                   # Frontend UI (static or SPA entry)
â”‚
â”œâ”€â”€ README.md                      # Project documentation
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .gitignore                     # Git ignore rules

```

## RAG System

The system maintains two FAISS indices:

1. **Story Index**: Stores CLIP embeddings of generated stories
2. **Image Index**: Stores CLIP embeddings and metadata of generated images

During generation:

* Retrieves top-K similar examples
* Provides context to generation models
* Improves consistency and quality over time

## Observability

Enable Langfuse for production monitoring:

```python
# config/settings.py
ENABLE_LANGFUSE = True
LANGFUSE_PUBLIC_KEY = "pk-..."
LANGFUSE_SECRET_KEY = "sk-..."
```

Tracks:

* Generation latency
* GPU memory usage
* Embedding extraction
* RAG retrieval performance
* Error rates

## Performance Tips

1. **Batch processing**: Queue multiple jobs for efficiency
2. **Model caching**: Keep frequently-used models loaded
3. **FAISS tuning**: Adjust `FAISS_NLIST` based on dataset size
4. **Worker scaling**: Add more workers for parallel processing
5. **GPU pinning**: Use separate GPUs for workers if available

## Troubleshooting

**Out of Memory**:

```bash
# Enable 8-bit quantization
USE_8BIT=true

# Or reduce image size
IMAGE_SIZE=512
```

**Slow generation**:

```bash
# Reduce inference steps
NUM_INFERENCE_STEPS=20

# Scale workers
docker-compose up --scale story_worker=3
```

**Redis connection failed**:

```bash
# Check Redis status
redis-cli ping

# Restart Redis
redis-server --daemonize yes
```

## ğŸ§ª Testing

```bash
# Test CLIP embeddings
python -c "from models.model_manager import ModelManager; m = ModelManager(); m.load_clip(); print('CLIP loaded!')"

# Test RAG
python -c "from rag.rag_engine import RAGEngine; from models.model_manager import ModelManager; m = ModelManager(); m.load_clip(); r = RAGEngine(m.clip_model, m.clip_processor, m.device); r.build_index(); print(f'RAG loaded with {len(r.examples)} examples')"
```

## ğŸ¤ Contributing

To add new features:

1. Models: Edit `models/model_manager.py`
2. RAG logic: Edit `rag/rag_engine.py`
3. API endpoints: Edit `main.py`

## ğŸ“ License

MIT License - Free for research and commercial use

---

**Questions?** Check FastAPI auto-docs at `/docs` endpoint!


<!-- Setting: The desolate, red-hued surface of the moon.
Subjects: A lone astronaut in a retro-futuristic spacesuit carefully plants a single, bright red flower in the lunar soil.
Atmosphere: A mix of isolation and hope. The Earth is visible as a blue and white marble in the inky black sky.
Keywords: Astronaut, planting flower, red moon, desolate, Earth in sky, hope, isolated, retro-futuristic. -->
